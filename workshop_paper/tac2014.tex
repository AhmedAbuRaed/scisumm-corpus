%
% File acl-hlt2011.tex
%
% Contact: gdzhou@suda.edu.cn
%%
%% Based on the style files for ACL2008 by Joakim Nivre and Noah Smith
%% and that of ACL2010 by Jing-Shin Chang and Philipp Koehn


\documentclass[11pt]{article}
\usepackage{tac2014}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{The Computational Linguistics Summarization Pilot Task}

\author{Kokil Jaidka \\
  Wee Kim Wee School of  \\
  Communication \& Information \\
 Nanyang Technological University \\
  {\tt koki0001@e.ntu.edu.sg} \\\And
  Muthu Kumar Chandrasekaran \\
  Dept. of Computer Science \\
  School of Computing \\
  National University of Singapore \\
  {\tt muthu.chandra@comp.nus.edu.sg} \\\And
  Min-Yen Kan \\
  Dept. of Computer Science \\
  School of Computing \\
  National University of Singapore \\
  {\tt kanmy@comp.nus.edu.sg} \\\And
  Ankur Khanna \\
  Web, IR \/ NLP Group \\
  School of Computing \\
  National University of Singapore \\
  {\tt khanna89ankur@gmail.com} 
  }

\date{}

\begin{document}
\maketitle
\begin{abstract}
\end{abstract}

\section{Introduction}
This paper describes the evolution and design of the SciSumm Shared Task for 
the scientific summarisation of computational linguistics research papers. 
It was concurrently publicized with the upcoming TAC 2014, although it is not 
formally affiliated with the same, and shares its basic structure and 
guidelines with the more formal BiomedSumm track of TAC 2014. A development 
corpus of training ``topics'' from computational linguistics (CL) research 
papers was released, each comprising a main, cited paper alongwith 
associated citing papers. Participants were invited to enter their systems 
in a task-based evaluation, similar to the one announced by BioMedSumm.

This paper will describe the participating systems and survey their results 
from the task-based evaluation.

\section{Background}
Recent works \cite{mohammad2009}\cite{abu2011} in scientific document 
summarisation have used citation sentences or citances from citing papers 
to create a multi document summary of the reference paper (RP). The 
computational linguistics (CL) community uses the ACL Anthology Reference 
Corpus \cite{bird2008} to evaluate and report performance of such systems. 
To support further research in this direction 
we built a manually annotated corpus of 10 randomly sampled documents from 
the ACL anthology reference corpus.

As proposed by \cite{vu2010}, \cite{hoang2010} the summarisation can be 
decomposed into finding the relevant documents, in this case, the citing 
papers (CPs), then selecting sentences from those papers that cite and 
justify the citation and finally generate the summary. To help tackle each 
of these subproblems, we created gold standard datasets where human 
annotators identify the citances in each of about 10 randomly sampled citing 
papers for the RP. 

Given a reference paper and up to 10 citing papers, annotators from National 
University of Singapore and Nanyang Technological University were instructed 
to find citations to the RP in the 10 CPs. Annotators followed instructions 
used for annotation of corpus for the TAC 2014 Biomedical Summarisation task 
(BiomedSumm) to encourage cross participation across the two tasks. 
Specifically, the citation text, citation marker, reference text, and discourse 
facet were marked for each citation of the RP found in the CP.

A pilot study conducted in the information science domain indicated that most 
citations clearly refer to one or more specific aspects of the cited paper 
\cite{jaidka2013}. For computational linguistics, we identified that the discourse 
facets being cited were usually the aim of the paper, methods followed and the 
results or implications of the work. Accordingly, we used a different set of 
discourse facets than BiomedSumm which suit CL papers better.	

Please note that this is a development corpus and only a training set is 
available for use now. Although, we plan to release a test set of documents 
for next year’s evaluation, we plan to report k fold cross-validated performance 
over the 10 documents for the two systems registered for participation.

\section{The Task}
In this task, we explore a new form of structured summary: a faceted summary 
of the traditional self-summary (the abstract) and the community summary 
(the collection of citances).  As a third component, we propose to group the 
citances by the facets of the text that they refer to. We propose that by 
identifying first, the cited text span, and second, the facet of the paper 
(Aim, Method, Result or Implication), we can create a faceted summary of the 
paper by clustering all cited/citing sentences together by facet.

The SciSumm Shared Task is defined as follows:

Given: A topic consisting of a Reference Paper (RP) and upto ten Citing 
Papers (CPs) that all contain citations to the RP. In each CP, the text spans 
(i.e., citances) have been identified that pertain to a particular citation 
to the RP.

Task 1a: For each citance, identify the spans of text (cited text spans) in 
the RP that most accurately reflect the citance. These are of the granularity 
of a sentence fragment, a full sentence, or several consecutive sentences 
(no more than 5).

Task 1b: For each cited text span, identify what facet of the paper it belongs 
to, from a predefined set of facets.

Evaluation: Task 1 will be scored by overlap of text spans in the system output 
vs the gold standard created by human annotators.

\section{Participating teams}
The following teams have expressed an interest in participating, and may be 
submitting their findings in this paper:

\begin{itemize}
\item{Taln.UPF, from Universitat Pompeu Fabra, Spain. They have proposed to 
		adapt available summarisation tools to scientific texts.}
\item{Clair\_UMICH from University of Michigan, Ann Arbor, USA.}
\item{IITKGP\_sum, from Indian Institute of Technology, Kharagpur, India. They 
plan to use citation network structure and citation context analysis to 
summarise the scientific articles.}
\item{CCS2014, from the IDA Center for Computing Sciences, USA. They will 
employ a language model based on the sections of the document to find 
referring text and related sentences in the cited document.}
\item{TabiBoun14, from the Boğaziçi University, Turkey. They plan to modify 
an existing system for CL papers, wherein they use LIBSVM as a classification 
tool for face classification. They also plan to use the cosine similarity 
metric to compare text spans.}
\item{PolyAF, from The Hong Kong Polytechnic University.}
\item{TXSUMM, from University of Houston, Texas. Their system consists of 
applying similarity kernels in an attempt to better discriminate between 
candidate text spans (with sentence granularity). They are using an 
extractive procedure with ranking algorithms.}
\item{MQ, from Macquarie University, Australia. They plan to use the same 
system that was used for the BiomedSumm track, with the exception that they 
will not incorporate domain knowledge (UMLS). For task 1a they will use 
similarity metrics to extract the top n sentences from the documents. For 
task 1b we will use a logistic regression classifier. For task 2 we will 
incorporate the distances from task 1 to rank the sentences.}
\item{A team from IHMC, USA}

\end{itemize}

\section*{Acknowledgments}

Do not number the acknowledgment section. Do not include this section when submitting your paper for review.

\bibliographystyle{tac2014}
\bibliography{tac2014}

\end{document}
