<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Several NLP tasks are characterized byasymmetric data where one class labelNONE, signifying the absence of anystructure (named entity, coreference, relation, etc.) dominates all other classes.Classifiers built on such data typicallyhave a higher precision and a lower recall and tend to overproduce the NONEclass.</S>
		<S sid ="2" ssid = "2">We present a novel scheme for voting among a committee of classifiers thatcan significantly boost the recall in suchsituations.</S>
		<S sid ="3" ssid = "3">We demonstrate results showing up to a 16% relative improvement inACE value for the 2004 ACE relation extraction task for English, Arabic and Chinese.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="4" ssid = "4">Statistical classifiers are widely used for diverseNLP applications such as part of speech tagging(Ratnaparkhi, 1999), chunking (Zhang et al., 2002),semantic parsing (Magerman, 1993), named entityextraction (Borthwick, 1999; Bikel et al., 1997; Florian et al., 2004), coreference resolution (Soon et al.,2001), relation extraction (Kambhatla, 2004), etc. Anumber of these applications are characterized by adominance of a NONE class in the training examples.</S>
			<S sid ="5" ssid = "5">For example, for coreference resolution, classifiers might classify whether a given pair of mentionsare references to the same entity or not.</S>
			<S sid ="6" ssid = "6">In this case,we typically have a lot more examples of mention pairs that are not coreferential (i.e. the NONE class)than otherwise.</S>
			<S sid ="7" ssid = "7">Similarly, if a classifier is predictingthe presence/absence of a semantic relation betweentwo mentions, there are typically far more examplessignifying an absence of a relation.</S>
			<S sid ="8" ssid = "8">Classifiers built with asymmetric data dominatedby one class (a NONE class donating absence of arelation or coreference or a named entity etc.) canovergenerate the NONE class.</S>
			<S sid ="9" ssid = "9">This often results in aunbalanced classifier where precision is higher thanrecall.</S>
			<S sid ="10" ssid = "10">In this paper, we present a novel approach forimproving the recall of such classifiers by using anew voting scheme from a committee of classifiers.There are a plethora of algorithms for combiningclassifiers (e.g. see (Xu et al., 1992)).</S>
			<S sid ="11" ssid = "11">A widelyused approach is a majority voting scheme, whereeach classifier in the committee gets a vote and theclass with the largest number of votes ’wins’ (i.e. thecorresponding class is output as the prediction of thecommittee).</S>
			<S sid ="12" ssid = "12">We are interested in improving overall recall andreduce the overproduction of the class NONE.</S>
			<S sid ="13" ssid = "13">Ourscheme predicts the class label C obtaining the second highest number of votes when NONE gets thehighest number of votes, provided C gets at leastN votes.</S>
			<S sid ="14" ssid = "14">Thus, we predict a label other than NONEwhen there is some evidence of the presense of thestructure we are looking for (relations, coreference,named entities, etc.) even in the absense of a clearmajority.This paper is organized as follows.</S>
			<S sid ="15" ssid = "15">In section 2,we give an overview of the various schemes for combining classifiers.</S>
			<S sid ="16" ssid = "16">In section 3, we present our vot 460 ing algorithm.</S>
			<S sid ="17" ssid = "17">In section 4, we describe the ACErelation extraction task.</S>
			<S sid ="18" ssid = "18">In section 5, we present empirical results for relation extraction and we discussour results and conclude in section 6.</S>
	</SECTION>
	<SECTION title="Combining Classifiers. " number = "2">
			<S sid ="19" ssid = "1">Numerous methods for combining classifiers havebeen proposed and utlized to improve the performance of different NLP tasks such as part of speechtagging (Brill and Wu, 1998), identifying base nounphrases (Tjong Kim Sang et al., 2000), named entity extraction (Florian et al., 2003), etc. Ho et al(1994) investigated different approaches for reranking the outputs of a committee of classifiers and alsoexplored union and intersection methods for reducing the set of predicted categories.</S>
			<S sid ="20" ssid = "2">Florian et al(2002) give a broad overview of methods for combining classifiers and present empirical results forword sense disambiguation.</S>
			<S sid ="21" ssid = "3">Xu et al (1992) and Florian et al (2002) considerthree approaches for combining classifiers.</S>
			<S sid ="22" ssid = "4">In thefirst approach, individual classifiers output posteriorprobabilities that are merged (e.g. by taking an average) to arrive at a composite posterior probabilityof each class.</S>
			<S sid ="23" ssid = "5">In the second scheme, each classifieroutputs a ranked list of classes instead of a probability distribution and the different ranked lists aremerged to arrive at a final ranking.</S>
			<S sid ="24" ssid = "6">Methods using the third approach, often called voting methods,treat each classifier as a black box that outputs onlythe top ranked class and combines these to arrive atthe final decision (class).</S>
			<S sid ="25" ssid = "7">The choice of approachand the specific method of combination may be constrained by the specific classification algorithms inuse.</S>
			<S sid ="26" ssid = "8">In this paper, we focus on voting methods, sincefor small data sets, it is hard to reliably estimateprobability distributions or even a complete ordering of classes especially when the number of classesis large.A widely used voting method for combining classifiers is a Majority Vote scheme (e.g.</S>
			<S sid ="27" ssid = "9">(Brill andWu, 1998; Tjong Kim Sang et al., 2000)).</S>
			<S sid ="28" ssid = "10">Eachclassifier gets to vote for its top ranked class andthe class with the highest number of votes ’wins’.Henderson et al (1999) use a Majority Vote schemewhere different parsers vote on constituents’ mem bership in a hypothesized parse.</S>
			<S sid ="29" ssid = "11">Halteren et al(1998) compare a number of voting methods including a Majority Vote scheme with other combinationmethods for part of speech tagging.</S>
			<S sid ="30" ssid = "12">In this paper, we induce multiple classifiers by using bagging (Breiman, 1996).</S>
			<S sid ="31" ssid = "13">Following Breiman’sapproach, we obtain multiple classifiers by firstmaking bootstrap replicates of the training data andtraining different classifiers on each of the replicates.The bootstrap replicates are induced by repeatedlysampling with replacement training events from theoriginal training data to arrive at replicate data setsof the same size as the training data set.</S>
			<S sid ="32" ssid = "14">Breiman(1996) uses a Majority Vote scheme for combiningthe output of the classifiers.</S>
			<S sid ="33" ssid = "15">In the next section, wewill describe the different voting schemes we explored in our work.</S>
	</SECTION>
	<SECTION title="At-Least-N Voting. " number = "3">
</PAPER>
