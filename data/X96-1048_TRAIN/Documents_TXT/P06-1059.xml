<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper presents techniques to applysemiCRFs to Named Entity Recognitiontasks with a tractable computational cost.Our framework can handle an NER taskthat has long named entities and manylabels which increase the computationalcost.</S>
		<S sid ="2" ssid = "2">To reduce the computational cost,we propose two techniques: the first is theuse of feature forests, which enables us topack feature-equivalent states, and the second is the introduction of a filtering process which significantly reduces the number of candidate states.</S>
		<S sid ="3" ssid = "3">This frameworkallows us to use a rich set of features extracted from the chunk-based representation that can capture informative characteristics of entities.</S>
		<S sid ="4" ssid = "4">We also introduce asimple trick to transfer information aboutdistant entities by embedding label information into nonentity labels.</S>
		<S sid ="5" ssid = "5">Experimental results show that our model achieves anF-score of 71.48% on the JNLPBA 2004shared task without using any external resources or post-processing techniques.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">The rapid increase of information in the biomedical domain has emphasized the need for automatedinformation extraction techniques.</S>
			<S sid ="7" ssid = "7">In this paperwe focus on the Named Entity Recognition (NER)task, which is the first step in tackling more complex tasks such as relation extraction and knowledge mining.</S>
			<S sid ="8" ssid = "8">Biomedical NER (BioNER) tasks are, in general, more difficult than ones in the news domain.For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al.,2004) was 72.55% (Zhou and Su, 2004) 1, whereasthe best performance at MUC6, in which systemstried to identify general named entities such asperson or organization names, was an accuracy of95% (Sundheim, 1995).</S>
			<S sid ="9" ssid = "9">Many of the previous studies of BioNER taskshave been based on machine learning techniquesincluding Hidden Markov Models (HMMs) (Bikelet al., 1997), the dictionary HMM model (Kou etal., 2005) and Maximum Entropy Markov Models (MEMMs) (Finkel et al., 2004).</S>
			<S sid ="10" ssid = "10">Among thesemethods, conditional random fields (CRFs) (Lafferty et al., 2001) have achieved good results (Kimet al., 2005; Settles, 2004), presumably becausethey are free from the so-called label bias problemby using a global normalization.</S>
			<S sid ="11" ssid = "11">Sarawagi and Cohen (2004) have recently introduced semi-Markov conditional random fields(semi-CRFs).</S>
			<S sid ="12" ssid = "12">They are defined on semi-Markovchains and attach labels to the subsequences of asentence, rather than to the tokens2.</S>
			<S sid ="13" ssid = "13">The semi-Markov formulation allows one to easily constructentity-level features.</S>
			<S sid ="14" ssid = "14">Since the features can capture all the characteristics of a subsequence, wecan use, for example, a dictionary feature whichmeasures the similarity between a candidate segment and the closest element in the dictionary.Kou et al.</S>
			<S sid ="15" ssid = "15">(2005) have recently showed that semi-CRFs perform better than CRFs in the task ofrecognition of protein entities.</S>
			<S sid ="16" ssid = "16">The main difficulty of applying semi-CRFs toBioNER lies in the computational cost at training 1Krauthammer (2004) reported that the inter-annotatoragreement rate of human experts was 77.6% for bioNLP,which suggests that the upper bound of the F-score in a BioNER task may be around 80%.</S>
			<S sid ="17" ssid = "17">2Assuming that nonentity words are placed in unit-lengthsegments.</S>
			<S sid ="18" ssid = "18">465 Table 1: Length distribution of entities in the training set of the shared task in 2004 JNLPBA Length # entity Ratio1 21646 42.192 15442 30.103 7530 14.684 3505 6.835 1379 2.696 732 1.437 409 0.808 252 0.49 &gt;8 406 0.79total 51301 100.00 because the number of named entity classes tendsto be large, and the training data typically containmany long entities, which makes it difficult to enumerate all the entity candidates in training.</S>
			<S sid ="19" ssid = "19">Table1 shows the length distribution of entities in thetraining set of the shared task in 2004 JNLPBA.Formally, the computational cost of training semi-CRFs is O(KLN), where L is the upper boundlength of entities, N is the length of sentence andK is the size of label set.</S>
			<S sid ="20" ssid = "20">And that of training infirst order semi-CRFs is O(K2LN).</S>
			<S sid ="21" ssid = "21">The increaseof the cost is used to transfer non-adjacent entityinformation.</S>
			<S sid ="22" ssid = "22">To improve the scalability of semi-CRFs, wepropose two techniques: the first is to introduce a filtering process that significantly reduces the number of candidate entities by usinga “lightweight” classifier, and the second is touse feature forest (Miyao and Tsujii, 2002), withwhich we pack the feature equivalent states.</S>
			<S sid ="23" ssid = "23">Theseenable us to construct semi-CRF models for thetasks where entity names may be long and manyclass-labels exist at the same time.</S>
			<S sid ="24" ssid = "24">We also presentan extended version of semi-CRFs in which wecan make use of information about a precedingnamed entity in defining features within the framework of first order semi-CRFs.</S>
			<S sid ="25" ssid = "25">Since the preceding entity is not necessarily adjacent to the currententity, we achieve this by embedding the information on preceding labels for named entities into thelabels for non-named entities.</S>
	</SECTION>
	<SECTION title="CRFs and Semi-CRFs. " number = "2">
			<S sid ="26" ssid = "1">CRFs are undirected graphical models that encodea conditional probability distribution using a given set of features.</S>
			<S sid ="27" ssid = "2">CRFs allow both discriminativetraining and bidirectional flow of probabilistic information along the sequence.</S>
			<S sid ="28" ssid = "3">In NER, we often use linear-chain CRFs, which define the conditional probability of a state sequence y = y1, ...,yn given the observed sequence x = x1,...,xn by: p(y|x, ?) = 1Z(x) exp(Sni=1Sj?jfj(yi1, yi, x, i)), (1)where fj(yi1, yi,x, i) is a feature function andZ(x) is the normalization factor over all the statesequences for the sequence x. The model parameters are a set of real-valued weights ? = {?j}, eachof which represents the weight of a feature.</S>
			<S sid ="29" ssid = "4">All thefeature functions are real-valued and can use adjacent label information.</S>
			<S sid ="30" ssid = "5">Semi-CRFs are actually a restricted version oforderLCRFs in which all the labels in a chunk arethe same.</S>
			<S sid ="31" ssid = "6">We follow the definitions in (Sarawagiand Cohen, 2004).</S>
			<S sid ="32" ssid = "7">Let s = &lt;s1, ..., sp&gt; denote asegmentation of x, where a segment sj = &lt;tj , uj ,yj&gt; consists of a start position tj , an end positionuj , and a label yj . We assume that segments have apositive length bounded above by the predefinedupper bound L (tj = uj , ujtj + 1 = L) andcompletely cover the sequence x without overlapping, that is, s satisfies t1 = 1, up = |x|, andtj+1 = uj + 1 for j = 1, ..., p - 1.</S>
			<S sid ="33" ssid = "8">Semi-CRFsdefine a conditional probability of a state sequencey given an observed sequence x by: p(y|x, ?) = 1Z(x) exp(SjSi?ifi(sj)), (2) where fi(sj) := fi(yj1, yj ,x, tj , uj) is a feature function and Z(x) is the normalization factoras defined for CRFs.</S>
			<S sid ="34" ssid = "9">The inference problem forsemiCRFs can be solved by using a semi-Markovanalog of the usual Viterbi algorithm.</S>
			<S sid ="35" ssid = "10">The computational cost for semi-CRFs is O(KLN) whereL is the upper bound length of entities, N is thelength of sentence and K is the number of labelset.</S>
			<S sid ="36" ssid = "11">If we use previous label information, the costbecomes O(K2LN).</S>
	</SECTION>
	<SECTION title="Using Non-Local Information inSemi-CRFs. " number = "3">
			<S sid ="37" ssid = "1">In conventional CRFs and semi-CRFs, one canonly use the information on the adjacent previous label when defining the features on a certainstate or entity.</S>
			<S sid ="38" ssid = "2">In NER tasks, however, information about a distant entity is often more useful than 466 O protein O O DNA O protein O-protein O-protein DNA Figure 1: Modification of “O” (other labels) totransfer information on a preceding named entity.</S>
			<S sid ="39" ssid = "3">information about the previous state (Finkel et al.,2005).</S>
			<S sid ="40" ssid = "4">For example, consider the sentence “... including Sp1 and CP1.” where the correct labels of“Sp1” and “CP1” are both “protein”.</S>
			<S sid ="41" ssid = "5">It would beuseful if the model could utilize the (non-adjacent)information about “Sp1” being “protein” to classify “CP1” as “protein”.</S>
			<S sid ="42" ssid = "6">On the other hand, information about adjacent labels does not necessarily provide useful information because, in manycases, the previous label of a named entity is “O”,which indicates a non-named entity.</S>
			<S sid ="43" ssid = "7">For 98.0% ofthe named entities in the training data of the sharedtask in the 2004 JNLPBA, the label of the preceding entity was “O”.</S>
			<S sid ="44" ssid = "8">In order to incorporate such non-local information into semi-CRFs, we take a simple approach.We divide the label of “O” into “O-protein” and“O” so that they convey the information on thepreceding named entity.</S>
			<S sid ="45" ssid = "9">Figure 1 shows an example of this conversion, in which the two labelsfor the third and fourth states are converted from“O” to “O-protein”.</S>
			<S sid ="46" ssid = "10">When we define the features for the fifth state, we can use the information on the preceding entity “protein” by looking at the fourth state.</S>
			<S sid ="47" ssid = "11">Since this modificationchanges only the label set, we can do this withinthe framework of semi-CRF models.</S>
			<S sid ="48" ssid = "12">This idea isoriginally proposed in (Peshkin and Pfeffer, 2003).However, they used a dynamic Bayesian network(DBNs) rather than a semi-CRF, and semi-CRFsare likely to have significantly better performancethan DBNs.In previous work, such non-local informationhas usually been employed at a post-processingstage.</S>
			<S sid ="49" ssid = "13">This is because the use of long distancedependency violates the locality of the model andprevents us from using dynamic programmingtechniques in training and inference.</S>
			<S sid ="50" ssid = "14">Skip-CRFs(Sutton and McCallum, 2004) are a direct imple mentation of long distance effects to the model.However, they need to determine the structurefor propagating non-local information in advance.In a recent study by Finkel et al., (2005), non-local information is encoded using an independence model, and the inference is performed byGibbs sampling, which enables us to use a state-of-the-art factored model and carry out training efficiently, but inference still incurs a considerablecomputational cost.</S>
			<S sid ="51" ssid = "15">Since our model handles limited type of non-local information, i.e. the labelof the preceding entity, the model can be solvedwithout approximation.</S>
	</SECTION>
	<SECTION title="Reduction of Training/Inference Cost. " number = "4">
</PAPER>
