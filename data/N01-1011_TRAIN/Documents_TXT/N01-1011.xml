<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper presents a corpus-based approach toword sense disambiguation where a decision tree assigns a sense to an ambiguous word based on thebigrams that occur nearby.</S>
		<S sid ="2" ssid = "2">This approach is evaluated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise.</S>
		<S sid ="3" ssid = "3">It is more accurate than the average results reported for 30 of 36 words, and is more accurate than the best results for 19 of 36 words.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="4" ssid = "4">Word sense disambiguation is the process of selecting the most appropriate meaning for a word, based on the context in which it occurs.</S>
			<S sid ="5" ssid = "5">For our purposes it is assumed that the set of possible meanings, i.e., the sense inventory, has already been determined.</S>
			<S sid ="6" ssid = "6">Forexample, suppose bill has the following set of possible meanings: a piece of currency, pending legisla tion, or a bird jaw.</S>
			<S sid ="7" ssid = "7">When used in the context of The Senate bill is under consideration, a human reader immediately understands that bill is being used in the legislative sense.</S>
			<S sid ="8" ssid = "8">However, a computer program attempting to perform the same task faces a di∆cult problem since it does not have the bene?t of innate common{sense or linguistic knowledge.Rather than attempting to provide computer pro grams with real{world knowledge comparable to that of humans, natural language processing has turned to corpus{based methods.</S>
			<S sid ="9" ssid = "9">These approachesuse techniques from statistics and machine learn ing to induce models of language usage from largesamples of text.</S>
			<S sid ="10" ssid = "10">These models are trained to perform particular tasks, usually via supervised learning.</S>
			<S sid ="11" ssid = "11">This paper describes an approach where a deci sion tree is learned from some number of sentences where each instance of an ambiguous word has been manually annotated with a sense{tag that denotes the most appropriate sense for that context.</S>
			<S sid ="12" ssid = "12">Prior to learning, the sense{tagged corpus must beconverted into a more regular form suitable for auto matic processing.</S>
			<S sid ="13" ssid = "13">Each sense{tagged occurrence ofan ambiguous word is converted into a feature vec tor, where each feature represents some property of the surrounding text that is considered to be relevant to the disambiguation process.</S>
			<S sid ="14" ssid = "14">Given the exibilityand complexity of human language, there is poten tially an in?nite set of features that could be utilized.However, in corpus{based approaches features usually consist of information that can be readily identi?ed in the text, without relying on extensive exter nal knowledge sources.</S>
			<S sid ="15" ssid = "15">These typically include the part{of{speech of surrounding words, the presence of certain key words within some window of context, and various syntactic properties of the sentence and the ambiguous word.</S>
			<S sid ="16" ssid = "16">The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text.</S>
			<S sid ="17" ssid = "17">The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated.We take this approach since surface lexical fea tures like bigrams, collocations, and co{occurrencesoften contribute a great deal to disambiguation accuracy.</S>
			<S sid ="18" ssid = "18">It is not clear how much disambiguation ac curacy is improved through the use of features that are identi?ed by more complex pre{processing suchas part{of{speech tagging, parsing, or anaphora res olution.</S>
			<S sid ="19" ssid = "19">One of our objectives is to establish a clearupper bounds on the accuracy of disambiguation us ing feature sets that do not impose substantial pre{ processing requirements.</S>
			<S sid ="20" ssid = "20">This paper continues with a discussion of our methods for identifying the bigrams that should be included in the feature set for learning.</S>
			<S sid ="21" ssid = "21">Then the decision tree learning algorithm is described, as aresome benchmark learning algorithms that are included for purposes of comparison.</S>
			<S sid ="22" ssid = "22">The experimen tal data is discussed, and then the empirical resultsare presented.</S>
			<S sid ="23" ssid = "23">We close with an analysis of our ?nd ings and a discussion of related work.</S>
	</SECTION>
	<SECTION title="Building a Feature Set of Bigrams. " number = "2">
			<S sid ="24" ssid = "1">We have developed an approach to word sense dis ambiguation that represents text entirely in terms of the occurrence of bigrams, which we de?ne to be two cat :cat totals big n 11 = 10 n 12 = 20 n 1+ = 30 :big n 21 = 40 n 22 = 930 n 2+ = 970 totals n +1 =50 n +2 =950 n ++ =1000 Figure 1: Representation of Bigram Countsconsecutive words that occur in a text.</S>
			<S sid ="25" ssid = "2">The distributional characteristics of bigrams are fairly consis tent across corpora; a majority of them only occur one time.</S>
			<S sid ="26" ssid = "3">Given the sparse and skewed nature ofthis data, the statistical methods used to select interesting bigrams must be carefully chosen.</S>
			<S sid ="27" ssid = "4">We ex plore two alternatives, the power divergence family of goodness of ?t statistics and the Dice Coe∆cient,an information theoretic measure related to point wise Mutual Information.</S>
			<S sid ="28" ssid = "5">Figure 1 summarizes the notation for word and bigram counts used in this paper by way of a 2 ? 2 contingency table.</S>
			<S sid ="29" ssid = "6">The value of n 11 shows how many times the bigram big cat occurs in the corpus.</S>
			<S sid ="30" ssid = "7">The value of n 12 shows how often bigrams occur where big is the ?rst word and cat is not the second.</S>
			<S sid ="31" ssid = "8">The counts in n +1 and n 1+ indicate how often words big and cat occur as the ?rst and second words of any bigram in the corpus.</S>
			<S sid ="32" ssid = "9">The total number of bigrams in the corpus is represented by n ++ . 2.1 The Power Divergence Family.</S>
			<S sid ="33" ssid = "10">(Cressie and Read, 1984) introduce the power diver gence family of goodness of ?t statistics.</S>
			<S sid ="34" ssid = "11">A numberof well known statistics belong to this family, includ ing the likelihood ratio statisticG 2 and Pearson&apos;sX 2 statistic.</S>
			<S sid ="35" ssid = "12">These measure the divergence of the observed (n ij ) and expected (m ij ) bigram counts, where m ijis estimated based on the assumption that the com ponent words in the bigram occur together strictly by chance: m ij = n i+ ? n +j n ++ Given this value, G 2 and X 2 are calculated as: G 2 = 2 X i;j n ij ? log n ij m ij X 2 = X i;j (n ij ?m ij ) 2 m ij (Dunning, 1993) argues in favor of G 2 over X 2, es pecially when dealing with very sparse and skewed data distributions.</S>
			<S sid ="36" ssid = "13">However, (Cressie and Read, 1984) suggest that there are cases where Pearson&apos;s statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other.</S>
			<S sid ="37" ssid = "14">In light of this, (Pedersen, 1996) presents Fisher&apos;s exact test as an alternative since it does not rely on the distributional assumptions that underly both Pearson&apos;s test and the likelihood ratio.</S>
			<S sid ="38" ssid = "15">Unfortunately it is usually not clear which test is most appropriate for a particular sample of data.We take the following approach, based on the obser vation that all tests should assign approximately thesame measure of statistical signi?cance when the bi gram counts in the contingency table do not violate any of the distributional assumptions that underlythe goodness of ?t statistics.</S>
			<S sid ="39" ssid = "16">We perform tests us ing X 2 , G 2 , and Fisher&apos;s exact test for each bigram.</S>
			<S sid ="40" ssid = "17">If the resulting measures of statistical signi?cance di?er, then the distribution of the bigram counts iscausing at least one of the tests to become unreli able.</S>
			<S sid ="41" ssid = "18">When this occurs we rely upon the value from Fisher&apos;s exact test since it makes fewer assumptions about the underlying distribution of data.</S>
			<S sid ="42" ssid = "19">For the experiments in this paper, we identi?ed the top 100 ranked bigrams that occur more than 5 times in the training corpus associated with a word.</S>
			<S sid ="43" ssid = "20">There were no cases where rankings produced by G 2 , X 2 , and Fisher&apos;s exact test disagreed, which is not altogether surprising given that low frequency bigrams were excluded.</S>
			<S sid ="44" ssid = "21">Since all of these statistics produced the same rankings, hereafter we make no distinction among them and simply refer to them generically as the power divergence statistic.</S>
			<S sid ="45" ssid = "22">2.2 Dice Coe∆cient.</S>
			<S sid ="46" ssid = "23">The Dice Coe∆cient is a descriptive statistic that provides a measure of association among two wordsin a corpus.</S>
			<S sid ="47" ssid = "24">It is similar to pointwise Mutual Information, a widely used measure that was ?rst intro duced for identifying lexical relationships in (Church and Hanks, 1990).</S>
			<S sid ="48" ssid = "25">Pointwise Mutual Information can be de?ned as follows: MI(w 1 ; w 2 ) = log 2 n 11 ? n ++ n +1 ? n 1+ where w 1 and w 2 represent the two words that make up the bigram.Pointwise Mutual Information quanti?es how often two words occur together in a bigram (the nu merator) relative to how often they occur overall in the corpus (the denominator).</S>
			<S sid ="49" ssid = "26">However, there is a curious limitation to pointwise Mutual Information.</S>
			<S sid ="50" ssid = "27">A bigram w 1 w 2 that occurs n 11 times in the corpus, and whose component words w 1 and w 2 only occur as a part of that bigram, will result in increasingly strong measures of association as the value of n 11 decreases.</S>
			<S sid ="51" ssid = "28">Thus, the maximum pointwise MutualInformation in a given corpus will be assigned to bi grams that occur one time, and whose component words never occur outside that bigram.</S>
			<S sid ="52" ssid = "29">These are usually not the bigrams that prove most useful for disambiguation, yet they will dominate a ranked list as determined by pointwise Mutual Information.</S>
			<S sid ="53" ssid = "30">The Dice Coe∆cient overcomes this limitation, and can be de?ned as follows: Dice(w 1 ; w 2 ) = 2 ? n 11 n +1 + n 1+ When n 11 = n 1+ = n +1 the value of Dice(w 1 ; w 2 ) will be 1 for all values n 11 . When the value of n. 11 is less than either of the marginal totals (the moretypical case) the rankings produced by the Dice Co e∆cient are similar to those of Mutual Information.The relationship between pointwise Mutual Infor mation and the Dice Coe∆cient is also discussed in (Smadja et al., 1996).</S>
			<S sid ="54" ssid = "31">We have developed the Bigram Statistics Package to produce ranked lists of bigrams using a range of tests.</S>
			<S sid ="55" ssid = "32">This software is written in Perl and is freely available from www.d.umn.edu/~tpederse.</S>
	</SECTION>
	<SECTION title="Learning Decision Trees. " number = "3">
			<S sid ="56" ssid = "1">Decision trees are among the most widely used ma chine learning algorithms.</S>
			<S sid ="57" ssid = "2">They perform a general to speci?c search of a feature space, adding the most informative features to a tree structure as the search proceeds.</S>
			<S sid ="58" ssid = "3">The objective is to select a minimal set of features that e∆ciently partitions the feature space into classes of observations and assemble them into a tree.</S>
			<S sid ="59" ssid = "4">In our case, the observations are manuallysense{tagged examples of an ambiguous word in con text and the partitions correspond to the di?erent possible senses.</S>
			<S sid ="60" ssid = "5">Each feature selected during the search process is represented by a node in the learned decision tree.Each node represents a choice point between a number of di?erent possible values for a feature.</S>
			<S sid ="61" ssid = "6">Learning continues until all the training examples are ac counted for by the decision tree.</S>
			<S sid ="62" ssid = "7">In general, such a tree will be overly speci?c to the training data and not generalize well to new examples.</S>
			<S sid ="63" ssid = "8">Therefore learning is followed by a pruning step where some nodes are eliminated or reorganized to produce a tree that can generalize to new circumstances.</S>
			<S sid ="64" ssid = "9">Test instances are disambiguated by ?nding a path through the learned decision tree from the root to aleaf node that corresponds with the observed features.</S>
			<S sid ="65" ssid = "10">An instance of an ambiguous word is dis ambiguated by passing it through a series of tests, where each test asks if a particular bigram occurs in the available window of context.We also include three benchmark learning algorithms in this study: the majority classi?er, the de cision stump, and the Naive Bayesian classi?er.</S>
			<S sid ="66" ssid = "11">The majority classi?er assigns the most common sense in the training data to every instance in the test data.</S>
			<S sid ="67" ssid = "12">A decision stump is a one node decisiontree(Holte, 1993) that is created by stopping the de cision tree learner after the single most informative feature is added to the tree.</S>
			<S sid ="68" ssid = "13">The Naive Bayesian classi?er (Duda and Hart, 1973) is based on certain blanket assumptions about the interactions among features in a corpus.</S>
			<S sid ="69" ssid = "14">There is no search of the feature space performed to build a representative model as is the case with decisiontrees.</S>
			<S sid ="70" ssid = "15">Instead, all features are included in the classi ?er and assumed to be relevant to the task at hand.</S>
			<S sid ="71" ssid = "16">There is a further assumption that each feature is conditionally independent of all other features, given the sense of the ambiguous word.</S>
			<S sid ="72" ssid = "17">It is most often used with a bag of words feature set, where everyword in the training sample is represented by a bi nary feature that indicates whether or not it occurs in the window of context surrounding the ambiguous word.We use the Weka (Witten and Frank, 2000) imple mentations of the C4.5 decision tree learner (known as J48), the decision stump, and the Naive Bayesianclassi?er.</S>
			<S sid ="73" ssid = "18">Weka is written in Java and is freely avail able from www.cs.waikato.ac.nz/~ml.</S>
	</SECTION>
	<SECTION title="Experimental Data. " number = "4">
			<S sid ="74" ssid = "1">Our empirical study utilizes the training and test data from the 1998 SENSEVAL evaluation of wordsense disambiguation systems.</S>
			<S sid ="75" ssid = "2">Ten teams partic ipated in the supervised learning portion of thisevent.</S>
			<S sid ="76" ssid = "3">Additional details about the exercise, in cluding the data and results referred to in this paper, can be found at the SENSEVAL web site(www.itri.bton.ac.uk/events/senseval/) and in (Kil garri?</S>
			<S sid ="77" ssid = "4">and Palmer, 2000).</S>
			<S sid ="78" ssid = "5">We included all 36 tasks from SENSEVAL for which training and test data were provided.</S>
			<S sid ="79" ssid = "6">Each task requires that the occurrences of a particular word in the test data be disambiguated based on a model learned from the sense{tagged instances in the training data.</S>
			<S sid ="80" ssid = "7">Some words were used in multiple tasks as di?erent parts of speech.</S>
			<S sid ="81" ssid = "8">For example, there were two tasks associated with bet, one for its use as a noun and the other as a verb.</S>
			<S sid ="82" ssid = "9">Thus, there are 36 tasks involving the disambiguation of 29 di?erent words.</S>
			<S sid ="83" ssid = "10">The words and part of speech associated with each task are shown in Table 1 in column 1.</S>
			<S sid ="84" ssid = "11">Note that the parts of speech are encoded as n for noun, a for adjective, v for verb, and p for words where the part of speech was not provided.</S>
			<S sid ="85" ssid = "12">The number of test and training instances for each task are shown in columns 2 and 4.</S>
			<S sid ="86" ssid = "13">Each instance consists of the sentence in which the ambiguous word occurs as well as one or two surrounding sentences.</S>
			<S sid ="87" ssid = "14">In general the total context available for each ambiguous word is less than 100 surrounding words.</S>
			<S sid ="88" ssid = "15">The number of distinct senses in the test data for each task is shown in column 3.</S>
	</SECTION>
	<SECTION title="Experimental Method. " number = "5">
			<S sid ="89" ssid = "1">The following process is repeated for each task.</S>
			<S sid ="90" ssid = "2">Cap italization and punctuation are removed from the training and test data.</S>
			<S sid ="91" ssid = "3">Two feature sets are selected from the training data based on the top 100 ranked bigrams according to the power divergence statisticand the Dice Coe∆cient.</S>
			<S sid ="92" ssid = "4">The bigram must have oc curred 5 or more times to be included as a feature.This step ?lters out a large number of possible bi grams and allows the decision tree learner to focus on a small number of candidate bigrams that are likely to be helpful in the disambiguation process.The training and test data are converted to feature vectors where each feature represents the occur rence of one of the bigrams that belong in the feature set.</S>
			<S sid ="93" ssid = "5">This representation of the training data is the actual input to the learning algorithms.</S>
			<S sid ="94" ssid = "6">Decision tree and decision stump learning is performed twice, once using the feature set determined by the power divergence statistic and again using the feature setidenti?ed by the Dice Coe∆cient.</S>
			<S sid ="95" ssid = "7">The majority clas si?er simply determines the most frequent sense in the training data and assigns that to all instances in the test data.</S>
			<S sid ="96" ssid = "8">The Naive Bayesian classi?er is based on a feature set where every word that occurs 5 or more times in the training data is included as a feature.All of these learned models are used to disam biguate the test data.</S>
			<S sid ="97" ssid = "9">The test data is kept separate until this stage.</S>
			<S sid ="98" ssid = "10">We employ a ?ne grained scoringmethod, where a word is counted as correctly disam biguated only when the assigned sense tag exactlymatches the true sense tag.</S>
			<S sid ="99" ssid = "11">No partial credit is as signed for near misses.</S>
	</SECTION>
	<SECTION title="Experimental Results. " number = "6">
			<S sid ="100" ssid = "1">The accuracy attained by each of the learning algo rithms is shown in Table 1.</S>
			<S sid ="101" ssid = "2">Column 5 reports the accuracy of the majority classi?er, columns 6 and 7 show the best and average accuracy reported by the 10 participating SENSEVAL teams.</S>
			<S sid ="102" ssid = "3">The evaluation at SENSEVAL was based on precision and recall, so we converted those scores to accuracy by taking their product.</S>
			<S sid ="103" ssid = "4">However, the best precision and recall may have come from di?erent teams, so the best accuracy shown in column 6 may actually be higher than that of any single participating SENSEVAL system.</S>
			<S sid ="104" ssid = "5">The average accuracy in column 7 is the product of theaverage precision and recall reported for the par ticipating SENSEVAL teams.</S>
			<S sid ="105" ssid = "6">Column 8 shows the accuracy of the decision tree using the J48 learningalgorithm and the features identi?ed by a power di vergence statistic.</S>
			<S sid ="106" ssid = "7">Column 10 shows the accuracy of the decision tree when the Dice Coe∆cient selects the features.</S>
			<S sid ="107" ssid = "8">Columns 9 and 11 show the accuracy of the decision stump based on the power divergencestatistic and the Dice Coe∆cient respectively.</S>
			<S sid ="108" ssid = "9">Fi nally, column 13 shows the accuracy of the Naive Bayesian classi?er based on a bag of words feature set.</S>
			<S sid ="109" ssid = "10">The most accurate method is the decision treebased on a feature set determined by the power di vergence statistic.</S>
			<S sid ="110" ssid = "11">The last line of Table 1 showsthe win-tie-loss score of the decision tree/power di vergence method relative to every other method.</S>
			<S sid ="111" ssid = "12">A win shows it was more accurate than the method in the column, a loss means it was less accurate, and a tie means it was equally accurate.</S>
			<S sid ="112" ssid = "13">The decision tree/power divergence method was more accurate than the best reported SENSEVAL results for 19 of the 36 tasks, and more accurate for 30 of the 36tasks when compared to the average reported accu racy.</S>
			<S sid ="113" ssid = "14">The decision stumps also fared well, proving to be more accurate than the best SENSEVAL results for 14 of the 36 tasks.</S>
			<S sid ="114" ssid = "15">In general the feature sets selected by the power divergence statistic result in more accurate decision trees than those selected by the Dice Coe∆cient.</S>
			<S sid ="115" ssid = "16">The power divergence tests prove to be more reliablesince they account for all possible events surround ing two words w 1 and w 2 ; when they occur as bigram w 1 w 2 , when w 1 or w 2 occurs in a bigram without the other, and when a bigram consists of neither.</S>
			<S sid ="116" ssid = "17">The Dice Coe∆cient is based strictly on the event where w 1 and w 2 occur together in a bigram.</S>
			<S sid ="117" ssid = "18">There are 6 tasks where the decision tree / powerdivergence approach is less accurate than the SENSEVAL average; promise-n, scrap-n, shirt-n, amazev, bitter-p, and sanction-p.</S>
			<S sid ="118" ssid = "19">The most dramatic difference occurred with amaze-v, where the SENSEVAL average was 92.4% and the decision tree accu racy was 58.6%.</S>
			<S sid ="119" ssid = "20">However, this was an unusual task where every instance in the test data belonged to a single sense that was a minority sense in the training data.</S>
	</SECTION>
	<SECTION title="Analysis of Experimental Results. " number = "7">
			<S sid ="120" ssid = "1">The characteristics of the decision trees and deci sion stumps learned for each word are shown in Table 2.</S>
			<S sid ="121" ssid = "2">Column 1 shows the word and part of speech.</S>
			<S sid ="122" ssid = "3">Columns 2, 3, and 4 are based on the feature set selected by the power divergence statistic whilecolumns 5, 6, and 7 are based on the Dice Coe∆ cient.</S>
			<S sid ="123" ssid = "4">Columns 2 and 5 show the node selected to serve as the decision stump.</S>
			<S sid ="124" ssid = "5">Columns 3 and 6 show the number of leaf nodes in the learned decision tree relative to the number of total nodes.</S>
			<S sid ="125" ssid = "6">Columns 4 and 7 show the number of bigram features selected Table 1: Experimental Results (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) senses j48 stump j48 stump naive word-pos test in test train maj best avg pow pow dice dice bayes accident-n 267 8 227 75.3 87.1 79.6 85.0 77.2 83.9 77.2 83.1 behaviour-n 279 3 994 94.3 92.9 90.2 95.7 95.7 95.7 95.7 93.2 bet-n 274 15 106 18.2 50.7 39.6 41.8 34.5 41.8 34.5 39.3 excess-n 186 8 251 1.1 75.9 63.7 65.1 38.7 60.8 38.7 64.5 oat-n 75 12 61 45.3 66.1 45.0 52.0 50.7 52.0 50.7 56.0 giant-n 118 7 355 49.2 67.6 56.6 68.6 59.3 66.1 59.3 70.3 knee-n 251 22 435 48.2 67.4 56.0 71.3 60.2 70.5 60.2 64.1 onion-n 214 4 26 82.7 84.8 75.7 82.7 82.7 82.7 82.7 82.2 promise-n 113 8 845 62.8 75.2 56.9 48.7 63.7 55.8 62.8 78.0 sack-n 82 7 97 50.0 77.1 59.3 80.5 58.5 80.5 58.5 74.4 scrap-n 156 14 27 41.7 51.6 35.1 26.3 16.7 26.3 16.7 26.7 shirt-n 184 8 533 43.5 77.4 59.8 46.7 43.5 51.1 43.5 60.9 amaze-v 70 1 316 0.0 100.0 92.4 58.6 12.9 60.0 12.9 71.4 bet-v 117 9 60 43.2 60.5 44.0 50.8 58.5 52.5 50.8 58.5 bother-v 209 8 294 75.0 59.2 50.7 69.9 55.0 64.6 55.0 62.2 bury-v 201 14 272 38.3 32.7 22.9 48.8 38.3 44.8 38.3 42.3 calculate-v 218 5 249 83.9 85.0 75.5 90.8 88.5 89.9 88.5 80.7 consume-v 186 6 67 39.8 25.2 20.2 36.0 34.9 39.8 34.9 31.7 derive-v 217 6 259 47.9 44.1 36.0 82.5 52.1 82.5 52.1 72.4 oat-v 229 16 183 33.2 30.8 22.5 30.1 22.7 30.1 22.7 56.3 invade-v 207 6 64 40.1 30.9 25.5 28.0 40.1 28.0 40.1 31.0 promise-v 224 6 1160 85.7 82.1 74.6 85.7 84.4 81.7 81.3 85.3 sack-v 178 3 185 97.8 95.6 95.6 97.8 97.8 97.8 97.8 97.2 scrap-v 186 3 30 85.5 80.6 68.6 85.5 85.5 85.5 85.5 82.3 seize-v 259 11 291 21.2 51.0 42.1 52.9 25.1 49.4 25.1 51.7 brilliant-a 229 10 442 45.9 31.7 26.5 55.9 45.9 51.1 45.9 58.1 oating-a 47 5 41 57.4 49.3 27.4 57.4 57.4 57.4 57.4 55.3 generous-a 227 6 307 28.2 37.5 30.9 44.9 32.6 46.3 32.6 48.9 giant-a 97 5 302 94.8 98.0 93.5 95.9 95.9 94.8 94.8 94.8 modest-a 270 9 374 61.5 49.6 44.9 72.2 64.4 73.0 64.4 68.1 slight-a 218 6 385 91.3 92.7 81.4 91.3 91.3 91.3 91.3 91.3 wooden-a 196 4 362 93.9 81.7 71.3 96.9 96.9 96.9 96.9 93.9 band-p 302 29 1326 77.2 81.7 75.9 86.1 84.4 79.8 77.2 83.1 bitter-p 373 14 144 27.0 44.6 39.8 36.4 31.3 36.4 31.3 32.6 sanction-p 431 7 96 57.5 74.8 62.4 57.5 57.5 57.1 57.5 56.8 shake-p 356 36 963 23.6 56.7 47.1 52.2 23.6 50.0 23.6 46.6 win-tie-loss (j48pow vs. X) 237-6 190-17 300-6 289-3 1415-7 289-3 241-11 to represent the training data.</S>
			<S sid ="126" ssid = "7">This table shows that there is little di?erence in the decision stump nodes selected from feature sets determined by the power divergence statistics versus the Dice Coe∆cient.</S>
			<S sid ="127" ssid = "8">This is to be expected since the top ranked bigrams for each measure are consistent, and the decision stump node is generally chosen from among those.</S>
			<S sid ="128" ssid = "9">However, there are di?erences between the feature sets selected by the power divergence statistics andthe Dice Coe∆cient.</S>
			<S sid ="129" ssid = "10">These are re ected in the dif ferent sized trees that are learned based on these feature sets.</S>
			<S sid ="130" ssid = "11">The number of leaf nodes and the total number of nodes for each learned tree is shown in columns 3 and 6.</S>
			<S sid ="131" ssid = "12">The number of internal nodes is simply the di?erence between the total nodes and the leaf nodes.</S>
			<S sid ="132" ssid = "13">Each leaf node represents the end of a path through the decision tree that makes a sense distinction.</S>
			<S sid ="133" ssid = "14">Since a bigram feature can onlyappear once in the decision tree, the number of inter Table 2: Decision Tree and Stump Characteristics power divergence dice coe∆cient (1) (2) (3) (4) (5) (6) (7) word-pos stump node leaf/total features stump node leaf/total features accident-n by accident 8/15 101 by accident 12/23 112 behaviour-n best behaviour 2/3 100 best behaviour 2/3 104 bet-n betting shop 20/39 50 betting shop 20/39 50 excess-n in excess 13/25 104 in excess 11/21 102 oat-n the oat 7/13 13 the oat 7/13 13 giant-n the giants 16/31 103 the giants 14/27 78 knee-n knee injury 23/45 102 knee injury 20/39 104 onion-n in the 1/1 7 in the 1/1 7 promise-n promise of 95/189 100 a promising 49/97 107 sack-n the sack 5/9 31 the sack 5/9 31 scrap-n scrap of 7/13 8 scrap of 7/13 8 shirt-n shirt and 38/75 101 shirt and 55/109 101 amaze-v amazed at 11/21 102 amazed at 11/21 102 bet-v i bet 4/7 10 i bet 4/7 10 bother-v be bothered 19/37 101 be bothered 20/39 106 bury-v buried in 28/55 103 buried in 32/63 103 calculate-v calculated to 5/9 103 calculated to 5/9 103 consume-v on the 4/7 20 on the 4/7 20 derive-v derived from 10/19 104 derived from 10/19 104 oat-v oated on 24/47 80 oated on 24/47 80 invade-v to invade 55/109 107 to invade 66/127 108 promise-v promise to 3/5 100 promise you 5/9 106 sack-v return to 1/1 91 return to 1/1 91 scrap-v of the 1/1 7 of the 1/1 7 seize-v to seize 26/51 104 to seize 57/113 104 brilliant-a a brilliant 26/51 101 a brilliant 42/83 103 oating-a in the 7/13 10 in the 7/13 10 generous-a a generous 57/113 103 a generous 56/111 102 giant-a the giant 2/3 102 a giant 1/1 101 modest-a a modest 14/27 101 a modest 10/19 105 slight-a the slightest 2/3 105 the slightest 2/3 105 wooden-a wooden spoon 2/3 104 wooden spoon 2/3 101 band-p band of 14/27 100 the band 21/41 117 bitter-p a bitter 22/43 54 a bitter 22/43 54 sanction-p south africa 12/23 52 south africa 12/23 52 shake-p his head 90/179 100 his head 81/161 105 nal nodes represents the number of bigram features selected by the decision tree learner.</S>
			<S sid ="134" ssid = "15">One of our original hypotheses was that accurate decision trees of bigrams will include a relatively small number of features.</S>
			<S sid ="135" ssid = "16">This was motivated bythe success of decision stumps in performing disam biguation based on a single bigram feature.</S>
			<S sid ="136" ssid = "17">In these experiments, there were no decision trees that used all of the bigram features identi?ed by the ?ltering step, and for many words the decision tree learner went on to eliminate most of the candidate features.This can be seen by comparing the number of inter nal nodes with the number of candidate features as shown in columns 4 or 7.</S>
			<S sid ="137" ssid = "18">1 It is also noteworthy that the bigrams ultimately selected by the decision tree learner for inclusion in the tree do not always include those bigrams ranked most highly by the power divergence statistic or the Dice Coe∆cient.</S>
			<S sid ="138" ssid = "19">This is to be expected, since theselection of the bigrams from raw text is only mea 1 For most words the 100 top ranked bigrams form the set of candidate features presented to the decision tree learner.</S>
			<S sid ="139" ssid = "20">If there are ties in the top 100 rankings then there may be morethan 100 features, and if the there were fewer than 100 bi grams that occurred more than 5 times then all such bigrams are included in the feature set.</S>
			<S sid ="140" ssid = "21">suring the association between two words, while the decision tree seeks bigrams that partition instances of the ambiguous word into into distinct senses.</S>
			<S sid ="141" ssid = "22">In particular, the decision tree learner makes decisions as to what bigram to include as nodes in the treeusing the gain ratio, a measure based on the over all Mutual Information between the bigram and a particular word sense.</S>
			<S sid ="142" ssid = "23">Finally, note that the smallest decision trees are functionally equivalent to our benchmark methods.</S>
			<S sid ="143" ssid = "24">A decision tree with 1 leaf node and no internalnodes (1/1) acts as a majority classi?er.</S>
			<S sid ="144" ssid = "25">A deci sion tree with 2 leaf nodes and 1 internal node (2/3) has the structure of a decision stump.</S>
	</SECTION>
	<SECTION title="Discussion. " number = "8">
			<S sid ="145" ssid = "1">One of our long-term objectives is to identify a coreset of features that will be useful for disambiguat ing a wide class of words using both supervised and unsupervised methodologies.</S>
			<S sid ="146" ssid = "2">We have presented an ensemble approach to wordsense disambiguation (Pedersen, 2000) where mul tiple Naive Bayesian classi?ers, each based on co{ occurrence features from varying sized windows ofcontext, is shown to perform well on the widely stud ied nouns interest and line.</S>
			<S sid ="147" ssid = "3">While the accuracy ofthis approach was as good as any previously pub lished results, the learned models were complex and di∆cult to interpret, in e?ect acting as very accurate black boxes.Our experience has been that variations in learn ing algorithms are far less signi?cant contributors to disambiguation accuracy than are variations inthe feature set.</S>
			<S sid ="148" ssid = "4">In other words, an informative fea ture set will result in accurate disambiguation when used with a wide range of learning algorithms, but there is no learning algorithm that can perform well given an uninformative or misleading set of features.Therefore, our focus is on developing and discover ing feature sets that make distinctions among wordsenses.</S>
			<S sid ="149" ssid = "5">Our learning algorithms must not only pro duce accurate models, but they should also shed new light on the relationships among features and allowus to continue re?ning and understanding our fea ture sets.</S>
			<S sid ="150" ssid = "6">We believe that decision trees meet these criteria.</S>
			<S sid ="151" ssid = "7">A wide range of implementations are available, and they are known to be robust and accurate across a range of domains.</S>
			<S sid ="152" ssid = "8">Most important, their structure is easy to interpret and may provide insights into the relationships that exist among features and more general rules of disambiguation.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "9">
			<S sid ="153" ssid = "1">Bigrams have been used as features for word sensedisambiguation, particularly in the form of colloca tions where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)).</S>
			<S sid ="154" ssid = "2">While some of the bigrams we identify are collocations that includethe word being disambiguated, there is no require ment that this be the case.Decision trees have been used in supervised learn ing approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)).</S>
			<S sid ="155" ssid = "3">In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech ofneighboring words, three collocations, and the mor phology of the ambiguous word.</S>
			<S sid ="156" ssid = "4">We believe thatthe approach in this paper is the ?rst time that de cision trees based strictly on bigram features have been employed.</S>
			<S sid ="157" ssid = "5">The decision list is a closely related approach thathas also been applied to word sense disambigua tion (e.g., (Yarowsky, 1994), (Wilks and Stevenson, 1998), (Yarowsky, 2000)).</S>
			<S sid ="158" ssid = "6">Rather than building and traversing a tree to perform disambiguation, a list isemployed.</S>
			<S sid ="159" ssid = "7">In the general case a decision list may suffer from less fragmentation during learning than de cision trees; as a practical matter this means that thedecision list is less likely to be over{trained.</S>
			<S sid ="160" ssid = "8">How ever, we believe that fragmentation also re ects on the feature set used for learning.</S>
			<S sid ="161" ssid = "9">Ours consists ofat most approximately 100 binary features.</S>
			<S sid ="162" ssid = "10">This re sults in a relatively small feature space that is not as likely to su?er from fragmentation as are larger spaces.</S>
	</SECTION>
	<SECTION title="Future Work. " number = "10">
			<S sid ="163" ssid = "1">There are a number of immediate extensions to thiswork.</S>
			<S sid ="164" ssid = "2">The ?rst is to ease the requirement that bi grams be made up of two consecutive words.</S>
			<S sid ="165" ssid = "3">Rather, we will search for bigrams where the component words may be separated by other words in the text.</S>
			<S sid ="166" ssid = "4">The second is to eliminate the ?ltering step by whichcandidate bigrams are selected by a power diver gence statistic.</S>
			<S sid ="167" ssid = "5">Instead, the decision tree learnerwould consider all possible bigrams.</S>
			<S sid ="168" ssid = "6">Despite increasing the danger of fragmentation, this is an interest ing issue since the bigrams judged most informative by the decision tree learner are not always ranked highly in the ?ltering step.</S>
			<S sid ="169" ssid = "7">In particular, we willdetermine if the ?ltering process ever eliminates bigrams that could be signi?cant sources of disam biguation information.</S>
			<S sid ="170" ssid = "8">In the longer term, we hope to adapt this approach to unsupervised learning, where disambiguation is performed without the bene?t of sense tagged text.</S>
			<S sid ="171" ssid = "9">We are optimistic that this is viable, since bigram features are easy to identify in raw text.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "11">
			<S sid ="172" ssid = "1">This paper shows that the combination of a simplefeature set made up of bigrams and a standard deci sion tree learning algorithm results in accurate word sense disambiguation.</S>
			<S sid ="173" ssid = "2">The results of this approach are compared with those from the 1998 SENSEVAL word sense disambiguation exercise and show thatthe bigram based decision tree approach is more ac curate than the best SENSEVAL results for 19 of 36 words.</S>
	</SECTION>
	<SECTION title="Acknowledgments. " number = "12">
			<S sid ="174" ssid = "1">The Bigram Statistics Package has been imple mented by Satanjeev Banerjee, who is supported bya Grant{in{Aid of Research, Artistry and Scholarship from the O∆ce of the Vice President for Re search and the Dean of the Graduate School of the University of Minnesota.</S>
			<S sid ="175" ssid = "2">We would like to thank the SENSEVAL organizers for making the data and results from the 1998 event freely available.</S>
			<S sid ="176" ssid = "3">The comments of three anonymous reviewers were very helpful in preparing the ?nal version of this paper.A preliminary version of this paper appears in (Ped ersen, 2001).</S>
	</SECTION>
</PAPER>
